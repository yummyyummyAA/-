# simple_demo.py - å•æ–‡ä»¶å¿«é€Ÿæ¼”ç¤º
from transformers import AutoTokenizer, AutoModelForCausalLM
import streamlit as st

@st.cache_resource
def load_model():
    return AutoTokenizer.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True), \
           AutoModelForCausalLM.from_pretrained("THUDM/chatglm3-6b", trust_remote_code=True).cpu()

tokenizer, model = load_model()

st.title("ğŸ§  æœ¬åœ° ChatGLM3-6B èŠå¤©æœºå™¨äººï¼ˆè¯¾ç¨‹è®¾è®¡æ¼”ç¤ºï¼‰")

if "history" not in st.session_state:
    st.session_state.history = []

for h in st.session_state.history:
    st.chat_message("user").write(h[0])
    st.chat_message("assistant").write(h[1])

if prompt := st.chat_input():
    st.chat_message("user").write(prompt)
    with st.chat_message("assistant"):
        with st.spinner("ç”Ÿæˆä¸­..."):
            inp = tokenizer(prompt, return_tensors="pt")
            output = model.generate(**inp, max_new_tokens=128)
            response = tokenizer.decode(output[0], skip_special_tokens=True)
            st.write(response)
            st.session_state.history.append((prompt, response))
